{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5BPd1IecgTcc"
      },
      "source": [
        "# Zadanie 5\n",
        "\n",
        "\n",
        "Celem ćwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
        "\n",
        "Następnie należy wytrenować perceptron wielowarstwowy do klasyfikacji zbioru danych [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór MNIST dostępny jest w pakiecie `scikit-learn`.\n",
        "\n",
        "Punktacja:\n",
        "1. Implementacja propagacji do przodu (`forward`) [1 pkt]\n",
        "2. Implementacja wstecznej propagacji (`backward`) [2 pkt]\n",
        "3. Przeprowadzenie eksperymentów na zbiorze MNIST, w tym:\n",
        "    1. Porównanie co najmniej dwóch architektur sieci [1 pkt]\n",
        "    2. Przetestowanie każdej architektury na conajmniej 3 ziarnach [1 pkt]\n",
        "    3. Wnioski [2.5 pkt]\n",
        "4. Jakość kodu [0.5 pkt]\n",
        "\n",
        "Polecane źródła - teoria + intuicja:\n",
        "1. [Karpathy, CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1](https://www.youtube.com/watch?v=i94OvYb6noo&ab_channel=AndrejKarpathy)\n",
        "2. [3 Blude one Brown, Backpropagation calculus | Chapter 4, Deep learning\n",
        "](https://www.youtube.com/watch?v=tIeHLnjs5U8&t=4s&ab_channel=3Blue1Brown)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_amount = 10000\n",
        "validation = 2000\n",
        "learning = 7000\n",
        "testing = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "oNmMDnkRgTcj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X (features): (70000, 784)\n",
            "Shape of y (labels): (70000,)\n",
            "Shape of Xl (features): (7000, 784)\n",
            "Shape of yl (labels): (7000,)\n",
            "Shape of Xv (features): (2000, 784)\n",
            "Shape of yv (labels): (2000,)\n",
            "Shape of Xt (features): (1000, 784)\n",
            "Shape of yt (labels): (1000,)\n"
          ]
        }
      ],
      "source": [
        "from abc import abstractmethod, ABC\n",
        "from typing import List\n",
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "\n",
        "x,y = (mnist.data, mnist.target)\n",
        "# Convert labels to integers (optional)\n",
        "y = y.astype(int)\n",
        "\n",
        "# Normalize the features (optional: scale pixel values to range [0, 1])\n",
        "x = x / 255.0\n",
        "x_learning, label_learning = np.array(x[:(learning)]), np.array(y[:(learning)])\n",
        "x_validation, label_validation = np.array(x[(learning):(learning+validation)]), np.array(y[(learning):(learning+validation)])\n",
        "x_testing, label_testing = np.array(x[(learning+validation):(learning+validation+testing)]), np.array(y[(learning+validation):(learning+validation+testing)])\n",
        "\n",
        "print(\"Shape of X (features):\", x.shape)\n",
        "print(\"Shape of y (labels):\", y.shape)\n",
        "print(\"Shape of Xl (features):\", x_learning.shape)\n",
        "print(\"Shape of yl (labels):\", label_learning.shape)\n",
        "print(\"Shape of Xv (features):\", x_validation.shape)\n",
        "print(\"Shape of yv (labels):\", label_validation.shape)\n",
        "print(\"Shape of Xt (features):\", x_testing.shape)\n",
        "print(\"Shape of yt (labels):\", label_testing.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_hot_encode(labels: np.ndarray, num_classes: int) -> np.ndarray:\n",
        "    one_hot = np.zeros((labels.size, num_classes))\n",
        "    one_hot[np.arange(labels.size), labels] = 1\n",
        "    return one_hot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "Tl6Mj5wogTcm"
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        self._learning_rate = 0.05\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, output_error_derivative: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @learning_rate.setter\n",
        "    def learning_rate(self, learning_rate):\n",
        "        assert 0 < learning_rate < 1, f\"Learning rate must be in (0, 1). Got: {learning_rate}\"\n",
        "        self._learning_rate = learning_rate\n",
        "\n",
        "class FullyConnected(Layer):\n",
        "    def __init__(self, input_size: int, output_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n",
        "        self.bias = np.random.randn(output_size) * 0.01\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.input = x\n",
        "        return x @ self.weights + self.bias\n",
        "\n",
        "    def backward(self, output_error_derivative: np.ndarray) -> np.ndarray:\n",
        "        if output_error_derivative.ndim == 1:\n",
        "            output_error_derivative = output_error_derivative.reshape(1, -1)\n",
        "\n",
        "        if self.input.ndim == 1:\n",
        "            self.input = self.input.reshape(1, -1)\n",
        "\n",
        "        importance_gradient = self.input.T @ output_error_derivative\n",
        "        importance_gradient = np.clip(importance_gradient, -1.0, 1.0)\n",
        "\n",
        "        bias_gradient = np.sum(output_error_derivative, axis=0, keepdims=False)\n",
        "        bias_gradient = np.clip(bias_gradient, -1.0, 1.0)\n",
        "\n",
        "        input_error_derivative = output_error_derivative @ self.weights.T\n",
        "\n",
        "        self.weights -= self.learning_rate * importance_gradient\n",
        "        self.bias -= self.learning_rate * bias_gradient\n",
        "\n",
        "        return input_error_derivative\n",
        "\n",
        "class LeakyReLU(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.input = x\n",
        "        return np.where(x < 0, 0.01 * x, x)\n",
        "\n",
        "    def backward(self, output_error_derivative: np.ndarray) -> np.ndarray:\n",
        "        relu_derivative = np.where(self.input < 0, 0.01, 1)\n",
        "        return output_error_derivative * relu_derivative\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.input = x\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def backward(self, output_error_derivative: np.ndarray) -> np.ndarray:\n",
        "        tanh_derivative = 1 - np.tanh(self.input) ** 2\n",
        "        return output_error_derivative * tanh_derivative\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.output = 1 / (1 + np.exp(-x))\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_error_derivative: np.ndarray) -> np.ndarray:\n",
        "        sigmoid_derivative = self.output * (1 - self.output)\n",
        "        return output_error_derivative * sigmoid_derivative\n",
        "\n",
        "class Softmax(Layer):\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = x - np.max(x, axis=-1, keepdims=True)\n",
        "        exp_x = np.exp(x)\n",
        "        return exp_x / (np.sum(exp_x, axis=-1, keepdims=True) + 1e-15)\n",
        "\n",
        "    def backward(self, output_error_derivative: np.ndarray) -> np.ndarray:\n",
        "        return output_error_derivative\n",
        "\n",
        "class Loss:\n",
        "    def __init__(self, loss_function: callable, loss_function_derivative: callable) -> None:\n",
        "        self.loss_function = loss_function\n",
        "        self.loss_function_derivative = loss_function_derivative\n",
        "\n",
        "    def loss(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
        "        return self.loss_function(y_pred, y_true)\n",
        "\n",
        "    def loss_derivative(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
        "        return self.loss_function_derivative(y_pred, y_true)\n",
        "\n",
        "class CrossEntropy(Loss):\n",
        "    def __init__(self):\n",
        "        super().__init__(self.cross_entropy, self.cross_entropy_derivative)\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
        "        epsilon = 1e-15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy_derivative(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
        "        return y_pred - y_true\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers: List[Layer], learning_rate: float) -> None:\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_func = None\n",
        "\n",
        "        for layer in self.layers:\n",
        "            layer.learning_rate = self.learning_rate\n",
        "\n",
        "    def compile(self, loss: Loss) -> None:\n",
        "        self.loss_func = loss\n",
        "\n",
        "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def fit(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int, batch_size: int, verbose: int = 0, training: int = 1) -> None:\n",
        "        \"\"\"Fit the network to the training data.\"\"\"\n",
        "        assert isinstance(x_train, np.ndarray), \"x_train must be a NumPy array\"\n",
        "        assert isinstance(y_train, np.ndarray), \"y_train must be a NumPy array\"\n",
        "\n",
        "        num_classes = np.max(y_train) + 1\n",
        "        y_train_one_hot = one_hot_encode(y_train, num_classes)\n",
        "        accuracy_during_epoches = []\n",
        "        lost_during_epoches = []\n",
        "        Saved_states_learning = []\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            correct_predictions = 0\n",
        "            num_batches = len(x_train) // batch_size\n",
        "\n",
        "            for i in range(0, len(x_train), batch_size):\n",
        "                x_batch = x_train[i: i + batch_size]\n",
        "                y_batch = y_train_one_hot[i: i + batch_size]\n",
        "\n",
        "                outputs = self.__call__(x_batch)\n",
        "\n",
        "                batch_loss = self.loss_func.loss(outputs, y_batch)\n",
        "                total_loss += batch_loss\n",
        "\n",
        "                correct_predictions += (outputs.argmax(axis=1) == y_batch.argmax(axis=1)).sum()\n",
        "                if(training):\n",
        "                    grad = self.loss_func.loss_derivative(outputs, y_batch)\n",
        "                    for layer in reversed(self.layers):\n",
        "                        grad = layer.backward(grad)\n",
        "            avg_accuracy = correct_predictions / len(x_train)\n",
        "            accuracy_during_epoches.append(avg_accuracy)\n",
        "            lost_during_epoches.append(total_loss)\n",
        "            Saved_states_learning.append(copy.deepcopy(self))\n",
        "            if verbose:\n",
        "                avg_loss = total_loss / num_batches\n",
        "\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
        "        return accuracy_during_epoches, lost_during_epoches, Saved_states_learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "sZTneKpngTco"
      },
      "source": [
        "# Eksperymenty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 0.9471, Accuracy: 0.7613\n",
            "Epoch 2/20, Loss: 0.3801, Accuracy: 0.8989\n",
            "Epoch 3/20, Loss: 0.3007, Accuracy: 0.9200\n",
            "Epoch 4/20, Loss: 0.2587, Accuracy: 0.9313\n",
            "Epoch 5/20, Loss: 0.2294, Accuracy: 0.9373\n",
            "Epoch 6/20, Loss: 0.2059, Accuracy: 0.9436\n",
            "Epoch 7/20, Loss: 0.1861, Accuracy: 0.9483\n",
            "Epoch 8/20, Loss: 0.1686, Accuracy: 0.9543\n",
            "Epoch 9/20, Loss: 0.1531, Accuracy: 0.9581\n",
            "Epoch 10/20, Loss: 0.1392, Accuracy: 0.9627\n",
            "Epoch 11/20, Loss: 0.1266, Accuracy: 0.9674\n",
            "Epoch 12/20, Loss: 0.1153, Accuracy: 0.9713\n",
            "Epoch 13/20, Loss: 0.1052, Accuracy: 0.9749\n",
            "Epoch 14/20, Loss: 0.0960, Accuracy: 0.9780\n",
            "Epoch 15/20, Loss: 0.0878, Accuracy: 0.9800\n",
            "Epoch 16/20, Loss: 0.0803, Accuracy: 0.9839\n",
            "Epoch 17/20, Loss: 0.0736, Accuracy: 0.9851\n",
            "Epoch 18/20, Loss: 0.0673, Accuracy: 0.9870\n",
            "Epoch 19/20, Loss: 0.0617, Accuracy: 0.9886\n",
            "Epoch 20/20, Loss: 0.0565, Accuracy: 0.9899\n"
          ]
        }
      ],
      "source": [
        "layer_accuracy = []\n",
        "layer_loss = []\n",
        "layers1 = [\n",
        "    FullyConnected(784, 128),\n",
        "    LeakyReLU(),\n",
        "    FullyConnected(128, 32),\n",
        "    LeakyReLU(),\n",
        "    FullyConnected(32, 10),\n",
        "    Softmax()\n",
        "]\n",
        "\n",
        "layers2 = [\n",
        "    FullyConnected(784, 64),\n",
        "    LeakyReLU(),\n",
        "    FullyConnected(64, 10),\n",
        "    Tanh()\n",
        "]\n",
        "layers = [layers1,layers2]\n",
        "SEEDS = [0, 42, 100]\n",
        "for layer_idx, layer in enumerate(layers):\n",
        "    seed_accuracies = []\n",
        "    seed_losses = []\n",
        "    seed_val_accuracies = []\n",
        "    seed_val_losses = []\n",
        "\n",
        "    for seed in SEEDS:\n",
        "        np.random.seed(seed)  # Set the random seed\n",
        "\n",
        "        # Initialize network\n",
        "        network = Network(layers=layer, learning_rate=0.001)\n",
        "        network.compile(CrossEntropy())\n",
        "\n",
        "        epochs = 50\n",
        "        batch_size = 100\n",
        "\n",
        "        # Train the network\n",
        "        network_accuracy, network_loss, networks_saved = network.fit(\n",
        "            x_train=x_learning,\n",
        "            y_train=label_learning,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0,\n",
        "            training=1\n",
        "        )\n",
        "\n",
        "        # Validate the network\n",
        "        network_accuracy_validation = []\n",
        "        network_loss_validation = []\n",
        "        network_stats = []\n",
        "\n",
        "        for network in networks_saved:\n",
        "            network_stats.append(network.fit(\n",
        "                x_train=x_validation,\n",
        "                y_train=label_validation,\n",
        "                epochs=1,\n",
        "                batch_size=batch_size,\n",
        "                verbose=0,\n",
        "                training=0\n",
        "            ))\n",
        "            network_accuracy_validation.append(network_stats[-1][0])\n",
        "            network_loss_validation.append(network_stats[-1][1])\n",
        "\n",
        "        # Store results for this seed\n",
        "        seed_accuracies.append(network_accuracy)\n",
        "        seed_losses.append(network_loss)\n",
        "        seed_val_accuracies.append(network_accuracy_validation)\n",
        "        seed_val_losses.append(network_loss_validation)\n",
        "\n",
        "    # Average results across seeds\n",
        "    avg_train_accuracy = np.mean(seed_accuracies, axis=0)\n",
        "    avg_train_loss = np.mean(seed_losses, axis=0)\n",
        "    avg_val_accuracy = np.mean(seed_val_accuracies, axis=0)\n",
        "    avg_val_loss = np.mean(seed_val_losses, axis=0)\n",
        "\n",
        "    # Store averaged results\n",
        "    layer_accuracy.append((avg_train_accuracy, avg_val_accuracy))\n",
        "    layer_loss.append((avg_train_loss, avg_val_loss))\n",
        "\n",
        "    # Plot accuracy and loss for this layer\n",
        "    epochs_range = range(1, len(avg_train_accuracy) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, avg_train_accuracy, label='Training Accuracy', marker='o')\n",
        "    plt.plot(epochs_range, avg_val_accuracy, label='Validation Accuracy', marker='s')\n",
        "    plt.title(f'Layer {layer_idx + 1} - Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(epochs_range)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, avg_train_loss, label='Training Loss', marker='o', color='red')\n",
        "    plt.plot(epochs_range, avg_val_loss, label='Validation Loss', marker='s', color='orange')\n",
        "    plt.title(f'Layer {layer_idx + 1} - Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xticks(epochs_range)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "    Best_network = networks_saved[np.argmin(avg_val_loss)]\n",
        "\n",
        "    network_testing = Best_network.fit(x_train=x_testing,\n",
        "    y_train=label_testing,\n",
        "    epochs=1,\n",
        "    batch_size = batch_size,\n",
        "    verbose=1,\n",
        "    training=0)\n",
        "    print(f\"our network accuracy {network_testing[0]} and our network lost {network_testing[1]}, which was achived in {np.argmin(avg_val_loss)} network epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "UQ_vyk1bgTcp"
      },
      "source": [
        "# Wnioski"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VxlayEjgTcq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
