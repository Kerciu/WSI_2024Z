{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "#Zadanie 4 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Implementacja funkcji entropii - **0.5 pkt**\n",
        "* Implementacja funkcji entropii zbioru - **0.5 pkt**\n",
        "* Implementacja funkcji information gain - **0.5 pkt**\n",
        "* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n",
        "* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "def entropy_func(class_count, num_samples):\n",
        "    \"\"\"\n",
        "    Calculates entropy for the given class division from the\n",
        "    Shannon entropy formula\n",
        "\n",
        "    H(C)=-(∑ i=1 to k) p(ci)⋅log2(p(ci))\n",
        "\n",
        "    where:\n",
        "    k - number of classes\n",
        "    p(ci) - probability of appearance of class\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    if num_samples == 0: return 0\n",
        "    \n",
        "    prob = [count / num_samples for count in class_count if count > 0]\n",
        "    entropy = -sum(p * math.log2(p) for p in prob)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "class Group:\n",
        "    def __init__(self, group_classes):\n",
        "        self.group_classes = group_classes\n",
        "        self.entropy = self.group_entropy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.group_classes)\n",
        "\n",
        "    def group_entropy(self):\n",
        "        num_samples = len(self)\n",
        "        return entropy_func(self.group_classes, num_samples)\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, split_feature=None, split_val=None, depth=None, child_node_a=None, child_node_b=None, val=None):\n",
        "        self.split_feature = split_feature\n",
        "        self.split_val = split_val\n",
        "        self.depth = depth\n",
        "        self.child_node_a = child_node_a\n",
        "        self.child_node_b = child_node_b\n",
        "        self.val = val\n",
        "\n",
        "    def predict(self, data):\n",
        "        if self.val is not None:\n",
        "            return self.val\n",
        "\n",
        "        if data[self.split_feature] > self.split_val:\n",
        "            return self.child_node_b.predict(data)\n",
        "        else:\n",
        "            return self.child_node_a.predict(data)    \n",
        "\n",
        "\n",
        "class DecisionTreeClassifier(object):\n",
        "    def __init__(self, max_depth):\n",
        "        self.depth = 0\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_split_entropy(group_a: Group, group_b: Group):\n",
        "        \"\"\"\n",
        "        Weighted average of groups\n",
        "        \"\"\"\n",
        "        total_samples = len(group_a) + len(group_b)\n",
        "        group_a_weight = len(group_a) / total_samples\n",
        "        group_b_weight = len(group_b) / total_samples\n",
        "\n",
        "        group_a_entropy = group_a.group_entropy()\n",
        "        group_b_entropy = group_b.group_entropy()\n",
        "\n",
        "        return group_a_entropy * group_a_weight + group_b_entropy * group_b_weight\n",
        "\n",
        "    def get_information_gain(self, parent_group, child_group_a, child_group_b):\n",
        "        \"\"\"\n",
        "        Diff between parent_group entropy and weighted average of child group entropy\n",
        "        \"\"\"\n",
        "        return parent_group.group_entropy() - DecisionTreeClassifier.get_split_entropy(child_group_a, child_group_b)\n",
        "\n",
        "    def get_best_feature_split(self, feature_values, classes):\n",
        "        \"\"\"\n",
        "        Take this feature that maximizes infGain\n",
        "        \"\"\"\n",
        "        max_infgain = float(\"-inf\")\n",
        "        best_split_val = None\n",
        "    \n",
        "        unique_vals = sorted(set(feature_values))\n",
        "\n",
        "        for split_val in unique_vals:\n",
        "            \n",
        "            # divide into two groups\n",
        "            group_a_classes = [classes[i] for i in range(len(classes)) if classes[i] <= split_val]\n",
        "            group_b_classes = [classes[i] for i in range(len(classes)) if classes[i] > split_val]\n",
        "\n",
        "            # create two groups based on division\n",
        "            child_group_a: Group = Group([sum(group_a_classes == cls) for cls in np.unique(classes)])\n",
        "            child_group_b: Group = Group([sum(group_b_classes == cls) for cls in np.unique(classes)])\n",
        "\n",
        "            # create parent group\n",
        "            parent_group: Group = Group([sum(classes == cls) for cls in np.unique(classes)])\n",
        "\n",
        "            inf_gain = self.get_information_gain(parent_group, child_group_a, child_group_b)\n",
        "\n",
        "            if inf_gain > max_infgain:\n",
        "                max_infgain = inf_gain\n",
        "                best_split_val = split_val\n",
        "        \n",
        "        return best_split_val, max_infgain\n",
        "\n",
        "\n",
        "    def get_best_split(self, data, classes):\n",
        "        \n",
        "        max_infgain = float(\"-inf\")\n",
        "        best_feature = -1\n",
        "        best_split_val = None\n",
        "\n",
        "        for feature_idx in range(len(data[0])):\n",
        "\n",
        "            feature_values = [row[feature_idx] for row in data]\n",
        "            split_val, inf_gain = self.get_best_feature_split(feature_values, classes)\n",
        "\n",
        "            if inf_gain > max_infgain:\n",
        "                max_infgain = inf_gain\n",
        "                best_feature = feature_idx\n",
        "                best_split_val = split_val\n",
        "        \n",
        "        return best_feature, best_split_val\n",
        "\n",
        "\n",
        "    def build_tree(self, data, classes, depth=0):\n",
        "        if len(set(classes)) == 1:\n",
        "            return Node(val=classes[0])\n",
        "        \n",
        "        majority_class = Counter(classes).most_common(1)[0][0]\n",
        "\n",
        "        if depth > self.max_depth or len(data) <= 1:\n",
        "            return Node( val=majority_class )\n",
        "        \n",
        "        best_feature, best_split_val = self.get_best_split(data, classes)\n",
        "\n",
        "        if best_feature == -1 or best_split_val is None:\n",
        "            return Node( val=majority_class )\n",
        "        \n",
        "        left_indicies = [i for i in range(len(data)) if data[i][best_feature] <= best_split_val]\n",
        "        right_indicies = [i for i in range(len(data)) if data[i][best_feature] > best_split_val]\n",
        "\n",
        "        if not left_indicies or not right_indicies:\n",
        "            return Node( val=majority_class )\n",
        "\n",
        "        left_data = [data[i] for i in left_indicies]\n",
        "        right_data = [data[i] for i in right_indicies]\n",
        "\n",
        "        left_classes = [classes[i] for i in left_indicies]\n",
        "        right_classes = [classes[i] for i in right_indicies]\n",
        "\n",
        "        left_tree = self.build_tree(left_data, left_classes, depth + 1)\n",
        "        right_tree = self.build_tree(right_data, right_classes, depth + 1)\n",
        "        \n",
        "        return Node(best_feature, best_split_val, depth, left_tree, right_tree)\n",
        "        \n",
        "\n",
        "    def predict(self, data):\n",
        "        return self.tree.predict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "U033RY1_YS8x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ground truth: 1, Prediction: 1\n",
            "Ground truth: 2, Prediction: 1\n",
            "Ground truth: 2, Prediction: 1\n",
            "Ground truth: 1, Prediction: 1\n",
            "Ground truth: 0, Prediction: 1\n",
            "Ground truth: 2, Prediction: 1\n",
            "Ground truth: 1, Prediction: 1\n",
            "Ground truth: 0, Prediction: 0\n",
            "Ground truth: 0, Prediction: 0\n",
            "Ground truth: 1, Prediction: 1\n",
            "Ground truth: 2, Prediction: 1\n",
            "Ground truth: 0, Prediction: 1\n",
            "Ground truth: 1, Prediction: 1\n",
            "Ground truth: 2, Prediction: 1\n",
            "Ground truth: 2, Prediction: 1\n",
            "Accuracy: 0.47\n"
          ]
        }
      ],
      "source": [
        "dc = DecisionTreeClassifier(3)\n",
        "dc.tree= dc.build_tree(x_train, y_train)\n",
        "\n",
        "correct = 0\n",
        "\n",
        "for sample, gt in zip(x_test, y_test):\n",
        "    prediction = dc.predict(sample)\n",
        "    print(f'Ground truth: {gt}, Prediction: {prediction}')\n",
        "    if prediction == gt: correct += 1\n",
        "\n",
        "accuracy = correct / len(y_test)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernel_info": {
      "name": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
